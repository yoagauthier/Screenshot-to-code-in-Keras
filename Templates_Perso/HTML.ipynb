{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HTML.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "djvRatPI4WS7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# HTML code generation from screenshots\n",
        "\n",
        "This code is forked from the [Emil Wallner code](https://github.com/emilwallner/Screenshot-to-code-in-Keras/blob/master/local/HTML/HTML.ipynb) on Github on HTML code generation and upgraded with :\n",
        "- Classical computer vision approach to detect the different elements on the screenshots\n",
        "- Some hyper-parameter tuning\n",
        "- A few printing functions to understand the inputs and outputs of each parts of the model.\n",
        "\n",
        "## Init of Google colab environnement\n",
        "This project has been run & trained on [Google colab](https://colab.research.google.com/notebooks/welcome.ipynb), allowing easy & free iterations on Tesla K80. The next few lines install the necessary librarires and mount  a google drive folder to enable this notebook to access other files (PNG screenshots and HTML files in our case)."
      ]
    },
    {
      "metadata": {
        "id": "VnRjQkZw5yZR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vmUcbSZG4kjD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Installing libraries, checking and cding to the good directory in Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "uanrIkfIsXoV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y libsm6 libxext6\n",
        "!pip uninstall -y opencv-python\n",
        "!pip install -U opencv-contrib-python\n",
        "import os\n",
        "os.chdir(\"drive/ECP/DeepLearning/HTML\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lbYA7MwS442v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "id": "NeFfkL1_uhAk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer, one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, load_model\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Embedding, TimeDistributed, RepeatVector, LSTM, CuDNNLSTM, concatenate , Input, Reshape, Dense, Flatten\n",
        "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
        "from keras.callbacks import Callback\n",
        "from IPython.display import clear_output\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import copy\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sa0BL1lT4-Ch",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Detecting features using InceptionResNet (trained on Imagenet)"
      ]
    },
    {
      "metadata": {
        "id": "-7HcM3BzswFM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the images and preprocess them for inception-resnet\n",
        "images = []\n",
        "all_filenames = listdir('images/')\n",
        "all_filenames.sort()\n",
        "print(all_filenames)\n",
        "for filename in all_filenames:\n",
        "    images.append(img_to_array(load_img('images/'+filename, target_size=(299, 299))))\n",
        "images = np.array(images, dtype=float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FC9f3iP61iJX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "images = preprocess_input(images)\n",
        "# Run the images through inception-resnet and extract the features without the classification layer\n",
        "IR2 = InceptionResNetV2(weights='imagenet', include_top=False)\n",
        "features = IR2.predict(images)\n",
        "\n",
        "# print(features.shape)\n",
        "# plt.imshow(features[3][:,:,2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DWCxC5TttLzo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Load and image, preprocess it for IR2, extract features and generate the HTML\n",
        "IMG_SIZE= 299\n",
        "test_image = img_to_array(load_img('test/865.png', target_size=(IMG_SIZE, IMG_SIZE)))\n",
        "test_image = np.array(test_image, dtype=float)\n",
        "test_image = preprocess_input(test_image)\n",
        "test_features = IR2.predict(np.array([test_image]))\n",
        "test_image2 = img_to_array(load_img('test/86.png', target_size=(IMG_SIZE, IMG_SIZE)))\n",
        "test_image2 = np.array(test_image, dtype=float)\n",
        "test_image2 = preprocess_input(test_image2)\n",
        "test_features2 = IR2.predict(np.array([test_image2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iccPgIZp5ZaS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Computer vision approach\n",
        "Here we try different methods to process the images and detect elements of the screenshots, instead of using the heavy InceeptionResNet"
      ]
    },
    {
      "metadata": {
        "id": "7pmvsH9V6WD-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loading images"
      ]
    },
    {
      "metadata": {
        "id": "otxZl9OS2O6z",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "images = []\n",
        "IMG_SIZE = 128\n",
        "\n",
        "all_filenames = listdir('images/')\n",
        "all_filenames.sort()\n",
        "for filename in all_filenames:\n",
        "    # Here images are of size 512 x 512\n",
        "    img = cv2.imread('images/' + filename) # possible flag : cv2.IMREAD_GRAYSCALE to load the image in grayscale\n",
        "    images.append(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZUfmmiqn6yZT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Keypoints detection using SIFT\n",
        "Be carefull, executing this code will induce error on the Keras model, since the Deep Learning Network should have the descriptors as input and not the features map.\n",
        "This is left here only for reference."
      ]
    },
    {
      "metadata": {
        "id": "CF3VMSAaDtsw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Compute the SIFT points\n",
        "NB_DESCRIPTORS = 90\n",
        "descriptors = []\n",
        "kps = []\n",
        "for img in images:\n",
        "    sift = cv2.xfeatures2d.SIFT_create(contrastThreshold=0.01, edgeThreshold=40)\n",
        "    kp, des = sift.detectAndCompute(img, None)\n",
        "    print(des.shape)\n",
        "    if des.shape[0] < NB_DESCRIPTORS:\n",
        "        des = np.concatenate([des, np.zeros((NB_DESCRIPTORS - des.shape[0], 128))], axis=0)\n",
        "    else:\n",
        "        des = des[:NB_DESCRIPTORS]\n",
        "    \n",
        "    descriptors.append(des)\n",
        "    kps.append(kp)\n",
        "    \n",
        "dummy = np.zeros((1,1))\n",
        "descriptors = np.array(descriptors)\n",
        "print(descriptors.shape)\n",
        "\n",
        "# using a gray scale to output the tones of the elements in the screenshot\n",
        "boundaries = [(i, i + 10) for i in range(0, 255, 10)]\n",
        "\n",
        "\n",
        "features = []\n",
        "for img in images:\n",
        "  im_features = []\n",
        "  kp = cv2.cornerHarris(img,2,3,0.04)\n",
        "  im_features.append(kp)\n",
        "  \n",
        "  for (lower, upper) in boundaries:\n",
        "    mask = cv2.inRange(img, lower, upper)\n",
        "    output = cv2.bitwise_and(img, img, mask = mask)\n",
        "    im_features.append(output)\n",
        "  \n",
        "  features.append(im_features)\n",
        "\n",
        "features = np.array(features)\n",
        "print(features.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BctmOJPBD3x3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Features detection using contours & downsampling"
      ]
    },
    {
      "metadata": {
        "id": "FsZYXVr22J9B",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# partially inspired from https://docs.opencv.org/3.1.0/d3/db4/tutorial_py_watershed.html\n",
        "def process_screenshot(img, low_thresh, high_thresh):\n",
        "\n",
        "    # converting the image to grayscale, applying threshold to detect all the\n",
        "    # elements of the screenshot (text, frames, links ...)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    _, thresh = cv2.threshold(gray, low_thresh, 255, cv2.THRESH_BINARY_INV)\n",
        "    _2, thresh2 = cv2.threshold(gray, high_thresh, 255, cv2.THRESH_BINARY)\n",
        "    thresholded = cv2.add(thresh, thresh2)\n",
        "    thresholded = cv2.bitwise_not(thresholded)\n",
        "\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    # dilating the foreground to know for sure which part of the image is in the\n",
        "    # background\n",
        "    sure_bg = cv2.dilate(thresholded, kernel, iterations=3)\n",
        "\n",
        "    # Finding sure foreground area by reducing the \"interfaces\" between different\n",
        "    # elements\n",
        "    dist_transform = cv2.distanceTransform(thresholded, cv2.DIST_L2, 5)\n",
        "    _3, sure_fg = cv2.threshold(dist_transform, 0.3 * dist_transform.max(), 255, 0)\n",
        "\n",
        "    # Finding unknown region (usefull only for watershed algorithm)\n",
        "    sure_fg = np.uint8(sure_fg)\n",
        "    unknown = cv2.subtract(sure_bg,sure_fg)\n",
        "    # Marker labelling\n",
        "    _4, markers = cv2.connectedComponents(sure_fg)\n",
        "\n",
        "    # Add one to all labels so that sure background is not 0, but 1\n",
        "    markers = markers+1\n",
        "\n",
        "    # Now, mark the region of unknown with zero, watershed the image and draw a\n",
        "    # red line around the elements\n",
        "    # NOTE : This is unused because not as efficient as expected\n",
        "    # markers[unknown == 255] = 0\n",
        "    # markers = cv2.watershed(img, markers)\n",
        "    # img[markers == -1] = [255, 0, 0]\n",
        "\n",
        "    # finding the contours of the elements of the screenshot\n",
        "    # experience the best results for the sure foreground (so we will not use\n",
        "    # watershed method)\n",
        "    _5, contours, hierarchy = cv2.findContours(sure_fg, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)    \n",
        "\n",
        "    detection_imgs = []\n",
        "    rois = []  # regions of interest\n",
        "    windows = []\n",
        "    img2 = copy.copy(img)\n",
        "    for contour in contours:\n",
        "        [x, y, w, h] = cv2.boundingRect(contour)\n",
        "        area = w * h\n",
        "\n",
        "        # discard areas that are too small (probably noise)\n",
        "        if h < 3 or w < 3:\n",
        "            continue\n",
        "\n",
        "        # drawing a pink rectangle around contour on original image\n",
        "        img2 = cv2.rectangle(img2, (x, y), (x + w, y + h), (255, 0, 255), 1)\n",
        "        roi = img2[y + 1:y + h, x + 1:x + w]\n",
        "\n",
        "        # drawing only the detected element of the screenshot with a black\n",
        "        # background\n",
        "        window = np.zeros((512, 512, 3))\n",
        "        window[y + 1:y + h, x + 1:x + w] = img2[y + 1:y + h, x + 1:x + w]\n",
        "\n",
        "        # saving the resuls and resetting the original image for the next elements\n",
        "        # to process\n",
        "        detection_imgs.append(img2)\n",
        "        rois.append(roi)\n",
        "        windows.append((window, area))\n",
        "        img2 = copy.copy(img)\n",
        "    \n",
        "    # we order the windows by area to ensure wa have the most important one first\n",
        "    windows.sort(key=lambda tup: tup[1])\n",
        "    windows = [tup[0] for tup in windows]\n",
        "\n",
        "    # Here we downscale our images and keep the important features (localization\n",
        "    # of the element in the screenshot and texture) by using a Gaussian\n",
        "    # convolution\n",
        "    # See https://docs.opencv.org/3.4.0/d4/d1f/tutorial_pyramids.html for more infos\n",
        "    features = []  # 8x8x3 images\n",
        "    for window in windows:\n",
        "        feature = window\n",
        "        while feature.shape[0] / 2 >= 8:\n",
        "            feature = cv2.pyrDown(feature)\n",
        "        features.append(feature)\n",
        "\n",
        "    return detection_imgs, rois, windows, features\n",
        "\n",
        "\n",
        "detection_imgs, rois, windows, features = process_screenshot(images[3], 51, 253)\n",
        "INDEX_EL = 25\n",
        "fig = plt.figure()\n",
        "fig.add_subplot(2, 2, 1)\n",
        "plt.imshow(rois[INDEX_EL])\n",
        "fig.add_subplot(2, 2, 2)\n",
        "plt.imshow(windows[INDEX_EL])\n",
        "fig.add_subplot(2, 2, 3)\n",
        "plt.imshow(features[INDEX_EL])\n",
        "fig.add_subplot(2, 2, 4)\n",
        "plt.imshow(detection_imgs[INDEX_EL])\n",
        "plt.show()\n",
        "\n",
        "LOW_THRESH = 51\n",
        "HIGH_THRESH = 253\n",
        "NB_ELEMENTS_WANTED = 100\n",
        "features = []\n",
        "_, _, _, feats1 = process_screenshot(images[0], 54, 221)\n",
        "_, _, _, feats2 = process_screenshot(images[1], 53, 253)\n",
        "_, _, _, feats3 = process_screenshot(images[2], 53, 253)\n",
        "_, _, _, feats4 = process_screenshot(images[3], 51, 253)\n",
        "\n",
        "feats1 = np.array(feats1)\n",
        "feats2 = np.array(feats2)\n",
        "feats3 = np.array(feats3)\n",
        "feats4 = np.array(feats4)\n",
        "print(feats1.shape)\n",
        "print(feats2.shape)\n",
        "print(feats3.shape)\n",
        "print(feats4.shape)\n",
        "\n",
        "features.append(feats1[:30])\n",
        "features.append(feats2[:30])\n",
        "features.append(feats3[:30])\n",
        "features.append(feats4[:30])\n",
        "\n",
        "features = np.array(features)\n",
        "features = features.transpose((0, 2, 3, 4, 1))\n",
        "features = features.reshape(4, 8, 8, -1)\n",
        "features.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X4aocuJO7ER6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tokenizing text & preparing image features"
      ]
    },
    {
      "metadata": {
        "id": "b89mtTSHswFU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# We will cap each input sequence to 100 tokens\n",
        "max_caption_len = 100\n",
        "# Initialize the function that will create our vocabulary \n",
        "tokenizer = Tokenizer(filters='', split=\" \", lower=False)\n",
        "\n",
        "# Read a document and return a string\n",
        "def load_doc(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# Load all the HTML files\n",
        "X = []\n",
        "all_filenames = listdir('html/')\n",
        "all_filenames.sort()\n",
        "print(all_filenames)\n",
        "for filename in all_filenames:\n",
        "    X.append(load_doc('html/'+filename))\n",
        "\n",
        "# Create the vocabulary from the html files\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "# new_dict = {}\n",
        "# for key in tokenizer.word_docs.keys():\n",
        "#   new_key = key + '>'\n",
        "#   new_dict[new_key] = tokenizer.word_docs[key]\n",
        "# tokenizer.word_docs = new_dict\n",
        "\n",
        "# Add +1 to leave space for empty words\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "# Translate each word in text file to the matching vocabulary index\n",
        "sequences = tokenizer.texts_to_sequences(X)\n",
        "# The longest HTML file\n",
        "max_length = max(len(s) for s in sequences)\n",
        "for s in sequences:\n",
        "  print(len(s))\n",
        "print(vocab_size)\n",
        "print(max_length)\n",
        "  \n",
        "# Intialize our final input to the model\n",
        "X, y, image_data = list(), list(), list()\n",
        "for img_no, seq in enumerate(sequences):\n",
        "    for i in range(1, len(seq)):\n",
        "        # Add the entire sequence to the input and only keep the next word for the output\n",
        "        in_seq, out_seq = seq[:i], seq[i]\n",
        "        # If the sentence is shorter than max_length, fill it up with empty words\n",
        "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "        # Map the output to one-hot encoding\n",
        "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "        # Add and image corresponding to the HTML file\n",
        "        image_data.append(features[img_no])\n",
        "        # Cut the input sentence to 100 tokens, and add it to the input data\n",
        "        X.append(in_seq[-max_caption_len:])\n",
        "        y.append(out_seq)\n",
        "\n",
        "X, y, image_data = np.array(X), np.array(y), np.array(image_data)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y2mOynQE7N7c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model definition"
      ]
    },
    {
      "metadata": {
        "id": "6bbvmiALswFc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Create the encoder\n",
        "image_features = Input(shape=(8, 8, 1536,))\n",
        "image_flat = Flatten()(image_features)\n",
        "image_flat = Dense(128, activation='relu')(image_flat)\n",
        "ir2_out = RepeatVector(max_caption_len)(image_flat)\n",
        "\n",
        "language_input = Input(shape=(max_caption_len,))\n",
        "language_model = Embedding(vocab_size, 200, input_length=max_caption_len)(language_input)\n",
        "language_model = CuDNNLSTM(256, return_sequences=True)(language_model)\n",
        "language_model = CuDNNLSTM(256, return_sequences=True)(language_model)\n",
        "language_model = TimeDistributed(Dense(128, activation='relu', name='context_features'))(language_model)\n",
        "\n",
        "# Create the decoder\n",
        "decoder = concatenate([ir2_out, language_model])\n",
        "decoder = CuDNNLSTM(512, return_sequences=False)(decoder)\n",
        "decoder_output = Dense(vocab_size, activation='softmax')(decoder)\n",
        "\n",
        "# Compile the model\n",
        "model = Model(inputs=[image_features, language_input], outputs=decoder_output)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cv_QJAAn7SLl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Util function "
      ]
    },
    {
      "metadata": {
        "id": "oda0osFfugPb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZNqeikN3M0Wm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length, verbose=1):\n",
        "    # seed the generation process\n",
        "    in_text = 'START'\n",
        "    # iterate over the whole length of the sequence\n",
        "    for i in range(900):\n",
        "        # integer encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0][-100:]\n",
        "        # pad input\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        # predict next word\n",
        "        yhat = model.predict([photo,sequence], verbose=0)\n",
        "        # convert probability to integer\n",
        "        yhat = np.argmax(yhat)\n",
        "        # map integer to word\n",
        "        word = word_for_id(yhat, tokenizer)\n",
        "        # stop if we cannot map the word\n",
        "        if word is None:\n",
        "            print(\"NONE\")\n",
        "            break\n",
        "        # append as input for generating the next word\n",
        "        in_text += ' ' + word\n",
        "        # Print the prediction\n",
        "        if verbose > 0:\n",
        "          print(' ' + word, end='')\n",
        "        # stop if we predict the end of the sequence\n",
        "        if word == 'END':\n",
        "            break\n",
        "    return in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w60lYPRcMCAN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# loads the html file, tokenize it and put each token in the return list\n",
        "def load_text(filepath):\n",
        "    text = []\n",
        "    whole_text = load_doc(filepath)\n",
        "    # Add text and wrap it in a start and end tag\n",
        "    syntax = '<START> ' + whole_text + ' <END>'\n",
        "#     syntax = whole_text\n",
        "    # Seperate each word with a space (like the tokenizer)\n",
        "    syntax = ' '.join(syntax.split())\n",
        "    # Add a space between each comma\n",
        "    syntax = syntax.replace(',', ' ,')\n",
        "    text.append(syntax)\n",
        "    return text\n",
        "\n",
        "# Evaluate model\n",
        "def evaluate_model(model, texts, photo, tokenizer, max_length, verbose=1):\n",
        "    actual, predicted = list(), list()\n",
        "    # step over the whole set\n",
        "    for i in range(len(texts)):\n",
        "        yhat = generate_desc(model, tokenizer, photo, max_length, verbose)\n",
        "        # store actual and predicted\n",
        "        if verbose >0:\n",
        "          print('\\n\\nReal---->\\n\\n' + texts[i])\n",
        "        actual.append([texts[i].split()])\n",
        "        predicted.append(yhat.split())\n",
        "    # calculate BLEU score\n",
        "    bleu = corpus_bleu(actual, predicted)\n",
        "    return bleu, actual, predicted\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kd4YJVCu1out",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#\"\"\"Draws a graph of the loss of each epoch. This is very usefull to know when your overfitting (when testing the NN for each x epoch)\"\"\"\n",
        "BLEU_STEP = 25\n",
        "class PlotLossesBinary(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.bleu_train = []\n",
        "        self.bleu_test =[]\n",
        "        \n",
        "        self.index = 0\n",
        "        self.max = 0 \n",
        "        \n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        if self.i % BLEU_STEP == 0:\n",
        "          texts = load_text('test/865.html')\n",
        "          bleu, actual, predicted = evaluate_model(self.model, texts, np.array(test_features), tokenizer, max_caption_len, verbose=0)\n",
        "          self.bleu_test.append(bleu)\n",
        "          \n",
        "          texts = load_text('test/86.html')\n",
        "          bleu, actual, predicted = evaluate_model(self.model, texts, np.array(test_features2), tokenizer, max_caption_len, verbose=0)\n",
        "          self.bleu_train.append(bleu)\n",
        "          print(bleu)\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        \n",
        "        plt.subplot(221)\n",
        "        plt.plot(self.x, self.losses, label=\"Training loss\")\n",
        "        plt.legend()\n",
        "        \n",
        "        plt.subplot(222)\n",
        "        plt.plot([el for el in self.x if el % BLEU_STEP == 0], self.bleu_train, label=\"BLEU train score\")\n",
        "        plt.plot([el for el in self.x if el % BLEU_STEP == 0], self.bleu_test, label=\"BLEU test score\")\n",
        "        plt.legend()\n",
        "        \n",
        "        plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n",
        "                    wspace=0.35)\n",
        "        plt.show();\n",
        "        self.i += 1\n",
        "\n",
        "plot_losses = PlotLossesBinary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eqhV3r5M7WRw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ]
    },
    {
      "metadata": {
        "id": "7uoWqR69swFg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Train the neural network\n",
        "# model = load_model('model.h5')\n",
        "model.fit([image_data, X], y, batch_size=64, shuffle=False, epochs=50, callbacks=[plot_losses], verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VctMNwViXIb-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.save('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yb4iLKcf7csk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generating the HTML code from the trained model & the test image"
      ]
    },
    {
      "metadata": {
        "id": "zA-mcRyJP2dZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "test_image = cv2.imread('test/865.png') # possible flag : cv2.IMREAD_GRAYSCALE to load the image in grayscale\n",
        "_, _, _, test_features = process_screenshot(test_image, 75, 253)\n",
        "test_features = np.array(test_features[:30])\n",
        "test_features = test_features.transpose((1, 2, 3, 0))\n",
        "test_features = test_features.reshape(8, 8, -1)\n",
        "print(test_features.shape)\n",
        "generate_desc(model, tokenizer, np.array([test_features]), max_caption_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SXzvd76FswF2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Load and image, preprocess it for IR2, extract features and generate the HTML\n",
        "test_image = img_to_array(load_img('test/865.png', target_size=(299, 299)))\n",
        "test_image = np.array(test_image, dtype=float)\n",
        "test_image = preprocess_input(test_image)\n",
        "test_features = IR2.predict(np.array([test_image]))\n",
        "generate_desc(model, tokenizer, np.array(test_features), 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TXqGOu2zj7Qc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "texts = load_text('test/865.html')\n",
        "bleu, actual, predicted = evaluate_model(model, texts, np.array(test_features), tokenizer, max_caption_len)\n",
        "print(bleu)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P1wWFC3DcvXa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Bonus : here we can generate the HTML markup features to see what they look like, by getting the intermediate model\n",
        "def generate_language_features(model, tokenizer, max_length):\n",
        "    # seed the generation process\n",
        "    in_text = 'START'\n",
        "    # iterate over the whole length of the sequence\n",
        "    for i in range(900):\n",
        "        # integer encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0][-max_caption_len:]\n",
        "        # pad input\n",
        "       \n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "\n",
        "        context_features = model.predict([sequence], verbose=0)\n",
        "        print(\"features\", context_features)\n",
        "    return\n",
        "\n",
        "intermediate_model = Model(inputs=language_input, outputs=language_model)\n",
        "generate_language_features(intermediate_model, tokenizer, 100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}